<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/umamba/nnunetv2/training/nnUNetTrainer/nnUNetTrainerMultiAttention.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/umamba/nnunetv2/training/nnUNetTrainer/nnUNetTrainerMultiAttention.py" />
              <option name="originalContent" value="import os&#10;import torch&#10;from torch import nn&#10;from torch.nn import DataParallel&#10;from torch.nn.parallel import DistributedDataParallel as DDP&#10;import torch.distributed as dist&#10;import numpy as np&#10;from dynamic_network_architectures.building_blocks.helper import get_matching_instancenorm, convert_dim_to_conv_op&#10;from dynamic_network_architectures.initialization.weight_init import InitWeights_He&#10;from dynamic_network_architectures.building_blocks.plain_conv_encoder import PlainConvEncoder&#10;from nnunetv2.training.nnUNetTrainer.nnUNetTrainer import nnUNetTrainer&#10;from nnunetv2.utilities.plans_handling.plans_handler import ConfigurationManager, PlansManager&#10;from nnunetv2.nets.attention_blocks import CoordAttentionBlock3D, MambaBlock3D, EnhancedMambaBlock3D&#10;from nnunetv2.nets.unet_decoder_with_attention import UNetDecoderWithAttention&#10;&#10;&#10;class PlainConvUNetWithMultiAttention(nn.Module):&#10;    def __init__(self,&#10;                 input_channels: int,&#10;                 n_stages: int,&#10;                 features_per_stage: tuple,&#10;                 conv_op: type,&#10;                 kernel_sizes: tuple,&#10;                 strides: tuple,&#10;                 n_conv_per_stage: tuple,&#10;                 num_classes: int,&#10;                 n_conv_per_stage_decoder: tuple,&#10;                 conv_bias: bool = False,&#10;                 norm_op: type = nn.InstanceNorm3d,&#10;                 norm_op_kwargs: dict = None,&#10;                 dropout_op: type = None,&#10;                 dropout_op_kwargs: dict = None,&#10;                 nonlin: type = nn.LeakyReLU,&#10;                 nonlin_kwargs: dict = None,&#10;                 deep_supervision: bool = False,&#10;                 encoder_attention: str = &quot;coord_attention&quot;,&#10;                 decoder_attention: str = &quot;cbam&quot;):&#10;        &quot;&quot;&quot;&#10;        带有多注意力机制的3D UNet实现&#10;        &quot;&quot;&quot;&#10;        super().__init__()&#10;        if isinstance(n_conv_per_stage, int):&#10;            n_conv_per_stage = [n_conv_per_stage] * n_stages&#10;        if isinstance(n_conv_per_stage_decoder, int):&#10;            n_conv_per_stage_decoder = [n_conv_per_stage_decoder] * (n_stages - 1)&#10;            &#10;        # 使用带有注意力机制的编码器&#10;        self.encoder = PlainConvEncoderWithAttention(&#10;            input_channels, n_stages, features_per_stage, conv_op, kernel_sizes, strides,&#10;            n_conv_per_stage, conv_bias, norm_op, norm_op_kwargs, dropout_op,&#10;            dropout_op_kwargs, nonlin, nonlin_kwargs, encoder_attention)&#10;        &#10;        # 使用带注意力机制的解码器&#10;        self.decoder = UNetDecoderWithAttention(&#10;            self.encoder, &#10;            num_classes, &#10;            n_conv_per_stage_decoder, &#10;            deep_supervision,&#10;            nonlin=nonlin,&#10;            nonlin_kwargs=nonlin_kwargs,&#10;            dropout_op=dropout_op,&#10;            dropout_op_kwargs=dropout_op_kwargs,&#10;            norm_op=norm_op,&#10;            norm_op_kwargs=norm_op_kwargs,&#10;            attention_type=decoder_attention&#10;        )&#10;&#10;    def forward(self, x):&#10;        skips = self.encoder(x)&#10;        result = self.decoder(skips)&#10;        return result&#10;&#10;    def compute_conv_feature_map_size(self, input_size):&#10;        assert len(input_size) == self.encoder.conv_op.dim, &quot;Input size format incorrect. Example: (64, 128, 128, 128) for 3D&quot;&#10;        output = self.encoder.compute_conv_feature_map_size(input_size)&#10;        output += self.decoder.compute_conv_feature_map_size(input_size)&#10;        return output&#10;&#10;&#10;class PlainConvEncoderWithAttention(PlainConvEncoder):&#10;    def __init__(self,&#10;                 input_channels: int,&#10;                 n_stages: int,&#10;                 features_per_stage: tuple,&#10;                 conv_op: type,&#10;                 kernel_sizes: tuple,&#10;                 strides: tuple,&#10;                 n_conv_per_stage: tuple,&#10;                 conv_bias: bool = False,&#10;                 norm_op: type = nn.InstanceNorm3d,&#10;                 norm_op_kwargs: dict = None,&#10;                 dropout_op: type = None,&#10;                 dropout_op_kwargs: dict = None,&#10;                 nonlin: type = nn.LeakyReLU,&#10;                 nonlin_kwargs: dict = None,&#10;                 attention_type: str = &quot;coord_attention&quot;):&#10;        &quot;&quot;&quot;&#10;        带有注意力机制的编码器&#10;        &quot;&quot;&quot;&#10;        super(PlainConvEncoder, self).__init__()&#10;        self.input_channels = input_channels&#10;        self.n_stages = n_stages&#10;        self.features_per_stage = features_per_stage&#10;        self.conv_op = conv_op&#10;        self.output_channels = features_per_stage&#10;        self.conv_bias = conv_bias&#10;        self.norm_op = norm_op&#10;        self.norm_op_kwargs = norm_op_kwargs&#10;        self.nonlin = nonlin&#10;        self.nonlin_kwargs = nonlin_kwargs&#10;        self.dropout_op = dropout_op&#10;        self.dropout_op_kwargs = dropout_op_kwargs&#10;        self.strides = strides&#10;        self.kernel_sizes = kernel_sizes&#10;        self.return_skips = True&#10;        &#10;        # 定义池化操作&#10;        self.stages = nn.ModuleList()&#10;        for s in range(n_stages):&#10;            stage = nn.Sequential()&#10;            &#10;            # 添加下采样层（除了第一阶段）&#10;            if s &gt; 0:&#10;                stage.add_module('downsample', &#10;                                 conv_op(features_per_stage[s-1], features_per_stage[s], &#10;                                        kernel_size=strides[s], stride=strides[s], bias=False))&#10;            &#10;            # 添加卷积块（带注意力机制）&#10;            for i in range(n_conv_per_stage[s]):&#10;                # 正确计算输入和输出通道数&#10;                in_channels = input_channels if s == 0 and i == 0 else features_per_stage[s]&#10;                &#10;                # 根据注意力类型添加相应的注意力块&#10;                if attention_type == &quot;coord_attention&quot;:&#10;                    stage.add_module(f'conv_block_{i}', &#10;                                     CoordAttentionBlock3D(in_channels, features_per_stage[s]))&#10;                elif attention_type == &quot;mamba&quot;:&#10;                    stage.add_module(f'conv_block_{i}', &#10;                                     MambaBlock3D(in_channels, features_per_stage[s]))&#10;                elif attention_type == &quot;enhanced_mamba&quot;:&#10;                    stage.add_module(f'conv_block_{i}', &#10;                                     EnhancedMambaBlock3D(in_channels, features_per_stage[s]))&#10;                else:  # 默认不使用注意力机制&#10;                    from dynamic_network_architectures.building_blocks.simple_conv_blocks import StackedConvBlocks&#10;                    stage.add_module(f'conv_block_{i}',&#10;                                     StackedConvBlocks(&#10;                                         n_conv_per_stage=1,&#10;                                         conv_op=conv_op,&#10;                                         input_channels=in_channels,&#10;                                         output_channels=features_per_stage[s],&#10;                                         kernel_size=kernel_sizes[s],&#10;                                         stride=1,&#10;                                         conv_bias=conv_bias,&#10;                                         norm_op=norm_op,&#10;                                         norm_op_kwargs=norm_op_kwargs,&#10;                                         dropout_op=dropout_op,&#10;                                         dropout_op_kwargs=dropout_op_kwargs,&#10;                                         nonlin=nonlin,&#10;                                         nonlin_kwargs=nonlin_kwargs&#10;                                     ))&#10;            &#10;            self.stages.append(stage)&#10;            &#10;        # 初始化权重&#10;        self.apply(InitWeights_He(1e-2))&#10;    def _create_conv_block(self, in_channels, out_channels):&#10;        &quot;&quot;&quot;&#10;        创建普通的卷积块&#10;        &quot;&quot;&quot;&#10;        conv_op = self.conv_op&#10;        norm_op = self.norm_op&#10;        nonlin = self.nonlin&#10;        &#10;        return nn.Sequential(&#10;            conv_op(in_channels, out_channels, kernel_size=3, padding=1, bias=self.conv_bias),&#10;            norm_op(out_channels, **self.norm_op_kwargs) if self.norm_op_kwargs else norm_op(out_channels),&#10;            nonlin(**self.nonlin_kwargs) if self.nonlin_kwargs else nonlin()&#10;        )&#10;        &#10;    def forward(self, x):&#10;        &quot;&quot;&quot;&#10;        重写forward方法以确保正确返回skip连接&#10;        &quot;&quot;&quot;&#10;        skips = []&#10;        for s in self.stages:&#10;            x = s(x)&#10;            if self.return_skips:&#10;                skips.append(x)&#10;        if self.return_skips:&#10;            return skips&#10;        else:&#10;            return x&#10;&#10;&#10;class nnUNetTrainerMultiAttention(nnUNetTrainer):&#10;    def __init__(self, plans: dict, configuration: str, fold: int, dataset_json: dict, unpack_dataset: bool = True,&#10;                 device: torch.device = torch.device('cuda')):&#10;        # ---- 先读取环境变量并设置默认属性，避免父类__init__里调用 _set_batch_size_and_oversample 时找不到属性 ----&#10;        self.encoder_attention = os.environ.get('ENCODER_ATTENTION', 'coord_attention')&#10;        self.decoder_attention = os.environ.get('DECODER_ATTENTION', 'cbam')&#10;        self.target_num_stages = int(os.environ.get('UNET_3D_STAGES', -1))&#10;        self.batch_size_factor = float(os.environ.get('BATCH_SIZE_FACTOR', 1.0))&#10;        # 父类初始化前先假设非DDP，父类里可能会调用 _set_batch_size_and_oversample&#10;        self.is_ddp = False&#10;        # 先调用父类完成基础初始化（其中会调用 _set_batch_size_and_oversample）&#10;        super().__init__(plans, configuration, fold, dataset_json, unpack_dataset, device)&#10;&#10;        # ---- 之后再确定DDP真实状态并打印 ----&#10;        self.is_ddp = dist.is_available() and dist.is_initialized()&#10;        if self.is_ddp:&#10;            self.local_rank = dist.get_rank()&#10;            print(f&quot;Local rank {self.local_rank}: DDP环境已初始化&quot;)&#10;            print(f&quot;  World size: {dist.get_world_size()}&quot;)&#10;            print(f&quot;  Backend: {dist.get_backend()}&quot;)&#10;        else:&#10;            self.local_rank = 0&#10;            print(f&quot;单GPU/非DDP模式，使用设备: {device}&quot;)&#10;&#10;        # 根据网络深度调整优化器超参数（若需要）&#10;        if self.target_num_stages &gt; 6:&#10;            self.initial_lr = 1e-3&#10;            self.weight_decay = 3e-5&#10;&#10;        # 若 batch_size_factor != 1，在父类初始化已经设置过 batch_size 的情况下我们再显式调整一次并提示&#10;        if self.batch_size_factor != 1.0:&#10;            original_batch_size = getattr(self, 'batch_size', None)&#10;            if original_batch_size is not None:&#10;                new_batch_size = max(1, int(original_batch_size * self.batch_size_factor))&#10;                if new_batch_size != self.batch_size:&#10;                    print(f&quot;根据因子再次调整batch size: {self.batch_size} -&gt; {new_batch_size}&quot;)&#10;                    self.batch_size = new_batch_size&#10;&#10;    def _set_batch_size_and_oversample(self):&#10;        &quot;&quot;&quot;覆写: 在父类__init__期间调用。需确保所用属性已存在。&quot;&quot;&quot;&#10;        batch_size_factor = getattr(self, 'batch_size_factor', 1.0)&#10;        is_ddp = getattr(self, 'is_ddp', False)&#10;&#10;        if not is_ddp:&#10;            original_batch_size = self.configuration_manager.batch_size&#10;            self.batch_size = max(1, int(original_batch_size * batch_size_factor))&#10;            print(f&quot;单GPU模式，batch size设置为: {self.batch_size} (factor={batch_size_factor})&quot;)&#10;        else:&#10;            original_batch_size = self.configuration_manager.batch_size&#10;            adjusted_batch_size = max(1, int(original_batch_size * batch_size_factor))&#10;            world_size = dist.get_world_size()&#10;            my_rank = dist.get_rank()&#10;&#10;            # 处理batch size 小于 GPU 数量：自动扩展或给出提示&#10;            if adjusted_batch_size &lt; world_size:&#10;                if my_rank == 0:&#10;                    print(f&quot;警告: 原始batch size={adjusted_batch_size} &lt; GPU数={world_size}。自动设置为{world_size} (每GPU 1)。\n&quot;&#10;                          f&quot;若需自定义请设置环境变量：export BATCH_SIZE_FACTOR=比例 或 修改plans中的batch size。&quot;)&#10;                adjusted_batch_size = world_size&#10;&#10;            global_batch_size = adjusted_batch_size&#10;            batch_size_per_GPU = int(np.ceil(global_batch_size / world_size))&#10;&#10;            sample_id_low = 0 if my_rank == 0 else np.sum([batch_size_per_GPU] * my_rank)&#10;            sample_id_high = np.sum([batch_size_per_GPU] * (my_rank + 1))&#10;            # 截断到 global_batch_size 避免越界&#10;            sample_id_high = min(sample_id_high, global_batch_size)&#10;&#10;            oversample = [True if not i &lt; round(global_batch_size * (1 - self.oversample_foreground_percent)) else False&#10;                          for i in range(global_batch_size)]&#10;&#10;            if sample_id_low &gt;= global_batch_size:  # 多余进程（极端情况下）&#10;                print(f&quot;Rank {my_rank}: 没有可分配样本，进程将空转。建议减少GPU或增大batch size。&quot;)&#10;                self.batch_size = 0&#10;                self.oversample_foreground_percent = 0.0&#10;                return&#10;&#10;            effective_span = sample_id_high - sample_id_low&#10;            if effective_span &lt; batch_size_per_GPU:&#10;                batch_size_per_GPU = effective_span&#10;&#10;            if sample_id_high / global_batch_size &lt; (1 - self.oversample_foreground_percent):&#10;                oversample_percent = 0.0&#10;            elif sample_id_low / global_batch_size &gt; (1 - self.oversample_foreground_percent):&#10;                oversample_percent = 1.0&#10;            else:&#10;                oversample_percent = sum(oversample[sample_id_low:sample_id_high]) / max(1, batch_size_per_GPU)&#10;&#10;            print(f&quot;DDP模式 - worker {my_rank}: global_batch {global_batch_size}, perGPU {batch_size_per_GPU}, oversample {oversample_percent:.3f} (factor={batch_size_factor})&quot;)&#10;            self.batch_size = batch_size_per_GPU&#10;            self.oversample_foreground_percent = oversample_percent&#10;&#10;    def run_training(self):&#10;        &quot;&quot;&quot;覆写，增加tqdm进度条 (仅rank0)。通过环境变量 PROGRESS_BAR=1 开启。&quot;&quot;&quot;&#10;        use_bar = os.environ.get('PROGRESS_BAR', '0').lower() in ('1', 'true', 'yes', 'y')&#10;        if use_bar:&#10;            try:&#10;                from tqdm import tqdm&#10;            except ImportError:&#10;                if self.local_rank == 0:&#10;                    print('未安装tqdm，关闭进度条。 pip install tqdm 可开启。')&#10;                use_bar = False&#10;        self.on_train_start()&#10;        for epoch in range(self.current_epoch, self.num_epochs):&#10;            self.on_epoch_start()&#10;            self.on_train_epoch_start()&#10;            train_outputs = []&#10;            if use_bar and self.local_rank == 0:&#10;                iter_obj = tqdm(range(self.num_iterations_per_epoch), desc=f'Epoch {epoch} [train]', leave=False)&#10;            else:&#10;                iter_obj = range(self.num_iterations_per_epoch)&#10;            for batch_id in iter_obj:&#10;                train_outputs.append(self.train_step(next(self.dataloader_train)))&#10;            self.on_train_epoch_end(train_outputs)&#10;            with torch.no_grad():&#10;                self.on_validation_epoch_start()&#10;                val_outputs = []&#10;                if use_bar and self.local_rank == 0:&#10;                    val_iter = tqdm(range(self.num_val_iterations_per_epoch), desc=f'Epoch {epoch} [val]', leave=False)&#10;                else:&#10;                    val_iter = range(self.num_val_iterations_per_epoch)&#10;                for batch_id in val_iter:&#10;                    val_outputs.append(self.validation_step(next(self.dataloader_val)))&#10;                self.on_validation_epoch_end(val_outputs)&#10;            self.on_epoch_end()&#10;        self.on_train_end()&#10;&#10;    @staticmethod&#10;    def build_network_architecture(plans_manager: PlansManager,&#10;                                   dataset_json,&#10;                                   configuration_manager: ConfigurationManager,&#10;                                   num_input_channels,&#10;                                   enable_deep_supervision: bool = True) -&gt; nn.Module:&#10;        &quot;&quot;&quot;&#10;        构建带有多种注意力机制的3D UNet网络&#10;        &#10;        网络深度可以通过环境变量 UNET_3D_STAGES 控制:&#10;        - -1 (默认): 自动根据patch size和GPU内存优化&#10;        - 4-7: 指定具体的网络阶段数&#10;        &#10;        注意力机制可以通过环境变量控制:&#10;        - ENCODER_ATTENTION: 编码器注意力机制 (&quot;coord_attention&quot;, &quot;mamba&quot;, &quot;enhanced_mamba&quot;, &quot;none&quot;)&#10;        - DECODER_ATTENTION: 解码器注意力机制 (&quot;cbam&quot;, &quot;coord_attention&quot;, &quot;se_block&quot;, &quot;eca&quot;, &quot;sa&quot;, &quot;self_attention&quot;)&#10;        &#10;        特点:&#10;        1. 集成多种注意力机制增强特征表示&#10;        2. 自动调整网络深度以适应3D任务&#10;        3. 优化特征图数量平衡精度和内存使用&#10;        &quot;&quot;&quot;&#10;        # 获取原始配置参数&#10;        original_num_stages = len(configuration_manager.conv_kernel_sizes)&#10;        dim = len(configuration_manager.conv_kernel_sizes[0])&#10;        conv_op = convert_dim_to_conv_op(dim)&#10;&#10;        label_manager = plans_manager.get_label_manager(dataset_json)&#10;&#10;        # 验证是否为3D任务&#10;        if dim != 3:&#10;            raise ValueError(&quot;此训练器专为3D分割任务设计，当前任务不是3D任务&quot;)&#10;        &#10;        # 获取目标阶段数（网络深度）&#10;        target_num_stages_env = int(os.environ.get('UNET_3D_STAGES', -1))&#10;        &#10;        # 获取注意力类型&#10;        encoder_attention = os.environ.get('ENCODER_ATTENTION', 'coord_attention')&#10;        decoder_attention = os.environ.get('DECODER_ATTENTION', 'cbam')&#10;        &#10;        if target_num_stages_env == -1:&#10;            # 自动调整网络深度，针对3D任务优化&#10;            patch_size = configuration_manager.patch_size&#10;            volume = np.prod(patch_size)  # 3D patch的体素数量&#10;            &#10;            # 根据3D patch体积确定合适的网络深度&#10;            if volume &lt; 64*64*64:  # 相对较小的patch&#10;                target_num_stages = 5&#10;            elif volume &lt; 128*128*128:  # 中等大小patch&#10;                target_num_stages = 6&#10;            else:  # 大型patch&#10;                target_num_stages = min(7, original_num_stages)&#10;        else:&#10;            # 使用指定的阶段数&#10;            target_num_stages = max(4, min(target_num_stages_env, 8))&#10;        &#10;        # 确保不超过原始配置的阶段数&#10;        target_num_stages = min(target_num_stages, original_num_stages)&#10;        &#10;        print(f&quot;3D UNet with Multi Attention配置:&quot;)&#10;        print(f&quot;  原始阶段数={original_num_stages}, 目标阶段数={target_num_stages}&quot;)&#10;        print(f&quot;  编码器注意力机制={encoder_attention}, 解码器注意力机制={decoder_attention}&quot;)&#10;        &#10;        # 针对3D任务优化特征图数量，避免显存不足&#10;        base_num_features = configuration_manager.UNet_base_num_features&#10;        max_num_features = configuration_manager.unet_max_num_features&#10;        &#10;        # 对于3D任务，适当减少特征图数量以节省显存&#10;        if target_num_stages &gt;= 6:&#10;            base_num_features = min(base_num_features, 24)  # 减少基础特征数&#10;            max_num_features = min(max_num_features, 256)   # 减少最大特征数&#10;        &#10;        features_per_stage = [min(base_num_features * 2 ** i, max_num_features) for i in range(target_num_stages)]&#10;        print(f&quot;特征图数量: {features_per_stage}&quot;)&#10;        &#10;        # 调整卷积核大小和步长配置&#10;        conv_kernel_sizes = configuration_manager.conv_kernel_sizes[:target_num_stages]&#10;        pool_op_kernel_sizes = configuration_manager.pool_op_kernel_sizes[:target_num_stages]&#10;        &#10;        # 针对3D任务优化卷积块数量&#10;        # 更深的网络减少每个阶段的卷积层数以控制计算复杂度&#10;        if hasattr(configuration_manager, 'n_conv_per_stage_encoder'):&#10;            n_conv_per_stage_encoder = configuration_manager.n_conv_per_stage_encoder[:target_num_stages]&#10;        else:&#10;            # 默认每个阶段2个卷积层&#10;            n_conv_per_stage_encoder = [2] * target_num_stages&#10;            &#10;        if hasattr(configuration_manager, 'n_conv_per_stage_decoder'):&#10;            n_conv_per_stage_decoder = configuration_manager.n_conv_per_stage_decoder[:target_num_stages-1]&#10;        else:&#10;            # 默认每个阶段2个卷积层&#10;            n_conv_per_stage_decoder = [2] * (target_num_stages - 1)&#10;            &#10;        # 对于更深的网络，减少部分阶段的卷积层数&#10;        if target_num_stages &gt;= 6:&#10;            # 减少最后几个阶段的卷积层数以控制计算量&#10;            for i in range(max(0, target_num_stages-2), target_num_stages):&#10;                n_conv_per_stage_encoder[i] = min(n_conv_per_stage_encoder[i], 2)&#10;&#10;        # 构建网络&#10;        model = PlainConvUNetWithMultiAttention(&#10;            input_channels=num_input_channels,&#10;            n_stages=target_num_stages,&#10;            features_per_stage=features_per_stage,&#10;            conv_op=conv_op,&#10;            kernel_sizes=conv_kernel_sizes,&#10;            strides=pool_op_kernel_sizes,&#10;            num_classes=label_manager.num_segmentation_heads,&#10;            n_conv_per_stage=n_conv_per_stage_encoder,&#10;            n_conv_per_stage_decoder=n_conv_per_stage_decoder,&#10;            conv_bias=True,&#10;            norm_op=get_matching_instancenorm(conv_op),&#10;            norm_op_kwargs={'eps': 1e-5, 'affine': True},&#10;            dropout_op=None,&#10;            dropout_op_kwargs=None,&#10;            nonlin=nn.LeakyReLU,&#10;            nonlin_kwargs={'inplace': True},&#10;            deep_supervision=enable_deep_supervision,&#10;            encoder_attention=encoder_attention,&#10;            decoder_attention=decoder_attention&#10;        )&#10;        &#10;        return model" />
              <option name="updatedContent" value="import os&#10;import torch&#10;from torch import nn&#10;from torch.nn import DataParallel&#10;from torch.nn.parallel import DistributedDataParallel as DDP&#10;import torch.distributed as dist&#10;import numpy as np&#10;from dynamic_network_architectures.building_blocks.helper import get_matching_instancenorm, convert_dim_to_conv_op&#10;from dynamic_network_architectures.initialization.weight_init import InitWeights_He&#10;from dynamic_network_architectures.building_blocks.plain_conv_encoder import PlainConvEncoder&#10;from nnunetv2.training.nnUNetTrainer.nnUNetTrainer import nnUNetTrainer&#10;from nnunetv2.utilities.plans_handling.plans_handler import ConfigurationManager, PlansManager&#10;from nnunetv2.nets.attention_blocks import CoordAttentionBlock3D, MambaBlock3D, EnhancedMambaBlock3D&#10;from nnunetv2.nets.unet_decoder_with_attention import UNetDecoderWithAttention&#10;&#10;&#10;class PlainConvUNetWithMultiAttention(nn.Module):&#10;    def __init__(self,&#10;                 input_channels: int,&#10;                 n_stages: int,&#10;                 features_per_stage: tuple,&#10;                 conv_op: type,&#10;                 kernel_sizes: tuple,&#10;                 strides: tuple,&#10;                 n_conv_per_stage: tuple,&#10;                 num_classes: int,&#10;                 n_conv_per_stage_decoder: tuple,&#10;                 conv_bias: bool = False,&#10;                 norm_op: type = nn.InstanceNorm3d,&#10;                 norm_op_kwargs: dict = None,&#10;                 dropout_op: type = None,&#10;                 dropout_op_kwargs: dict = None,&#10;                 nonlin: type = nn.LeakyReLU,&#10;                 nonlin_kwargs: dict = None,&#10;                 deep_supervision: bool = False,&#10;                 encoder_attention: str = &quot;coord_attention&quot;,&#10;                 decoder_attention: str = &quot;cbam&quot;):&#10;        &quot;&quot;&quot;&#10;        带有多注意力机制的3D UNet实现&#10;        &quot;&quot;&quot;&#10;        super().__init__()&#10;        if isinstance(n_conv_per_stage, int):&#10;            n_conv_per_stage = [n_conv_per_stage] * n_stages&#10;        if isinstance(n_conv_per_stage_decoder, int):&#10;            n_conv_per_stage_decoder = [n_conv_per_stage_decoder] * (n_stages - 1)&#10;            &#10;        # 使用带有注意力机制的编码器&#10;        self.encoder = PlainConvEncoderWithAttention(&#10;            input_channels, n_stages, features_per_stage, conv_op, kernel_sizes, strides,&#10;            n_conv_per_stage, conv_bias, norm_op, norm_op_kwargs, dropout_op,&#10;            dropout_op_kwargs, nonlin, nonlin_kwargs, encoder_attention)&#10;        &#10;        # 使用带注意力机制的解码器&#10;        self.decoder = UNetDecoderWithAttention(&#10;            self.encoder, &#10;            num_classes, &#10;            n_conv_per_stage_decoder, &#10;            deep_supervision,&#10;            nonlin=nonlin,&#10;            nonlin_kwargs=nonlin_kwargs,&#10;            dropout_op=dropout_op,&#10;            dropout_op_kwargs=dropout_op_kwargs,&#10;            norm_op=norm_op,&#10;            norm_op_kwargs=norm_op_kwargs,&#10;            attention_type=decoder_attention&#10;        )&#10;&#10;    def forward(self, x):&#10;        skips = self.encoder(x)&#10;        result = self.decoder(skips)&#10;        return result&#10;&#10;    def compute_conv_feature_map_size(self, input_size):&#10;        assert len(input_size) == self.encoder.conv_op.dim, &quot;Input size format incorrect. Example: (64, 128, 128, 128) for 3D&quot;&#10;        output = self.encoder.compute_conv_feature_map_size(input_size)&#10;        output += self.decoder.compute_conv_feature_map_size(input_size)&#10;        return output&#10;&#10;&#10;class PlainConvEncoderWithAttention(PlainConvEncoder):&#10;    def __init__(self,&#10;                 input_channels: int,&#10;                 n_stages: int,&#10;                 features_per_stage: tuple,&#10;                 conv_op: type,&#10;                 kernel_sizes: tuple,&#10;                 strides: tuple,&#10;                 n_conv_per_stage: tuple,&#10;                 conv_bias: bool = False,&#10;                 norm_op: type = nn.InstanceNorm3d,&#10;                 norm_op_kwargs: dict = None,&#10;                 dropout_op: type = None,&#10;                 dropout_op_kwargs: dict = None,&#10;                 nonlin: type = nn.LeakyReLU,&#10;                 nonlin_kwargs: dict = None,&#10;                 attention_type: str = &quot;coord_attention&quot;):&#10;        &quot;&quot;&quot;&#10;        带有注意力机制的编码器&#10;        &quot;&quot;&quot;&#10;        super(PlainConvEncoder, self).__init__()&#10;        self.input_channels = input_channels&#10;        self.n_stages = n_stages&#10;        self.features_per_stage = features_per_stage&#10;        self.conv_op = conv_op&#10;        self.output_channels = features_per_stage&#10;        self.conv_bias = conv_bias&#10;        self.norm_op = norm_op&#10;        self.norm_op_kwargs = norm_op_kwargs&#10;        self.nonlin = nonlin&#10;        self.nonlin_kwargs = nonlin_kwargs&#10;        self.dropout_op = dropout_op&#10;        self.dropout_op_kwargs = dropout_op_kwargs&#10;        self.strides = strides&#10;        self.kernel_sizes = kernel_sizes&#10;        self.return_skips = True&#10;        &#10;        # 定义池化操作&#10;        self.stages = nn.ModuleList()&#10;        for s in range(n_stages):&#10;            stage = nn.Sequential()&#10;            &#10;            # 添加下采样层（除了第一阶段）&#10;            if s &gt; 0:&#10;                stage.add_module('downsample', &#10;                                 conv_op(features_per_stage[s-1], features_per_stage[s], &#10;                                        kernel_size=strides[s], stride=strides[s], bias=False))&#10;            &#10;            # 添加卷积块（带注意力机制）&#10;            for i in range(n_conv_per_stage[s]):&#10;                # 正确计算输入和输出通道数&#10;                in_channels = input_channels if s == 0 and i == 0 else features_per_stage[s]&#10;                &#10;                # 根据注意力类型添加相应的注意力块&#10;                if attention_type == &quot;coord_attention&quot;:&#10;                    stage.add_module(f'conv_block_{i}', &#10;                                     CoordAttentionBlock3D(in_channels, features_per_stage[s]))&#10;                elif attention_type == &quot;mamba&quot;:&#10;                    stage.add_module(f'conv_block_{i}', &#10;                                     MambaBlock3D(in_channels, features_per_stage[s]))&#10;                elif attention_type == &quot;enhanced_mamba&quot;:&#10;                    stage.add_module(f'conv_block_{i}', &#10;                                     EnhancedMambaBlock3D(in_channels, features_per_stage[s]))&#10;                else:  # 默认不使用注意力机制&#10;                    from dynamic_network_architectures.building_blocks.simple_conv_blocks import StackedConvBlocks&#10;                    stage.add_module(f'conv_block_{i}',&#10;                                     StackedConvBlocks(&#10;                                         n_conv_per_stage=1,&#10;                                         conv_op=conv_op,&#10;                                         input_channels=in_channels,&#10;                                         output_channels=features_per_stage[s],&#10;                                         kernel_size=kernel_sizes[s],&#10;                                         stride=1,&#10;                                         conv_bias=conv_bias,&#10;                                         norm_op=norm_op,&#10;                                         norm_op_kwargs=norm_op_kwargs,&#10;                                         dropout_op=dropout_op,&#10;                                         dropout_op_kwargs=dropout_op_kwargs,&#10;                                         nonlin=nonlin,&#10;                                         nonlin_kwargs=nonlin_kwargs&#10;                                     ))&#10;            &#10;            self.stages.append(stage)&#10;            &#10;        # 初始化权重&#10;        self.apply(InitWeights_He(1e-2))&#10;    def _create_conv_block(self, in_channels, out_channels):&#10;        &quot;&quot;&quot;&#10;        创建普通的卷积块&#10;        &quot;&quot;&quot;&#10;        conv_op = self.conv_op&#10;        norm_op = self.norm_op&#10;        nonlin = self.nonlin&#10;        &#10;        return nn.Sequential(&#10;            conv_op(in_channels, out_channels, kernel_size=3, padding=1, bias=self.conv_bias),&#10;            norm_op(out_channels, **self.norm_op_kwargs) if self.norm_op_kwargs else norm_op(out_channels),&#10;            nonlin(**self.nonlin_kwargs) if self.nonlin_kwargs else nonlin()&#10;        )&#10;        &#10;    def forward(self, x):&#10;        &quot;&quot;&quot;&#10;        重写forward方法以确保正确返回skip连接&#10;        &quot;&quot;&quot;&#10;        skips = []&#10;        for s in self.stages:&#10;            x = s(x)&#10;            if self.return_skips:&#10;                skips.append(x)&#10;        if self.return_skips:&#10;            return skips&#10;        else:&#10;            return x&#10;&#10;&#10;class nnUNetTrainerMultiAttention(nnUNetTrainer):&#10;    def __init__(self, plans: dict, configuration: str, fold: int, dataset_json: dict, unpack_dataset: bool = True,&#10;                 device: torch.device = torch.device('cuda')):&#10;        # ---- 先读取环境变量并设置默认属性，避免父类__init__里调用 _set_batch_size_and_oversample 时找不到属性 ----&#10;        self.encoder_attention = os.environ.get('ENCODER_ATTENTION', 'coord_attention')&#10;        self.decoder_attention = os.environ.get('DECODER_ATTENTION', 'cbam')&#10;        self.target_num_stages = int(os.environ.get('UNET_3D_STAGES', -1))&#10;        self.batch_size_factor = float(os.environ.get('BATCH_SIZE_FACTOR', 1.0))&#10;        # 父类初始化前先假设非DDP，父类里可能会调用 _set_batch_size_and_oversample&#10;        self.is_ddp = False&#10;        # 先调用父类完成基础初始化（其中会调用 _set_batch_size_and_oversample）&#10;        super().__init__(plans, configuration, fold, dataset_json, unpack_dataset, device)&#10;&#10;        # ---- 之后再确定DDP真实状态并打印 ----&#10;        self.is_ddp = dist.is_available() and dist.is_initialized()&#10;        if self.is_ddp:&#10;            self.local_rank = dist.get_rank()&#10;            print(f&quot;Local rank {self.local_rank}: DDP环境已初始化&quot;)&#10;            print(f&quot;  World size: {dist.get_world_size()}&quot;)&#10;            print(f&quot;  Backend: {dist.get_backend()}&quot;)&#10;        else:&#10;            self.local_rank = 0&#10;            print(f&quot;单GPU/非DDP模式，使用设备: {device}&quot;)&#10;&#10;        # 根据网络深度调整优化器超参数（若需要）&#10;        if self.target_num_stages &gt; 6:&#10;            self.initial_lr = 1e-3&#10;            self.weight_decay = 3e-5&#10;&#10;        # 若 batch_size_factor != 1，在父类初始化已经设置过 batch_size 的情况下我们再显式调整一次并提示&#10;        if self.batch_size_factor != 1.0:&#10;            original_batch_size = getattr(self, 'batch_size', None)&#10;            if original_batch_size is not None:&#10;                new_batch_size = max(1, int(original_batch_size * self.batch_size_factor))&#10;                if new_batch_size != self.batch_size:&#10;                    print(f&quot;根据因子再次调整batch size: {self.batch_size} -&gt; {new_batch_size}&quot;)&#10;                    self.batch_size = new_batch_size&#10;&#10;    def _set_batch_size_and_oversample(self):&#10;        &quot;&quot;&quot;覆写: 在父类__init__期间调用。需确保所用属性已存在。&quot;&quot;&quot;&#10;        batch_size_factor = getattr(self, 'batch_size_factor', 1.0)&#10;        is_ddp = getattr(self, 'is_ddp', False)&#10;&#10;        if not is_ddp:&#10;            original_batch_size = self.configuration_manager.batch_size&#10;            self.batch_size = max(1, int(original_batch_size * batch_size_factor))&#10;            print(f&quot;单GPU模式，batch size设置为: {self.batch_size} (factor={batch_size_factor})&quot;)&#10;        else:&#10;            original_batch_size = self.configuration_manager.batch_size&#10;            adjusted_batch_size = max(1, int(original_batch_size * batch_size_factor))&#10;            world_size = dist.get_world_size()&#10;            my_rank = dist.get_rank()&#10;&#10;            # 处理batch size 小于 GPU 数量：自动扩展或给出提示&#10;            if adjusted_batch_size &lt; world_size:&#10;                if my_rank == 0:&#10;                    print(f&quot;警告: 原始batch size={adjusted_batch_size} &lt; GPU数={world_size}。自动设置为{world_size} (每GPU 1)。\n&quot;&#10;                          f&quot;若需自定义请设置环境变量：export BATCH_SIZE_FACTOR=比例 或 修改plans中的batch size。&quot;)&#10;                adjusted_batch_size = world_size&#10;&#10;            global_batch_size = adjusted_batch_size&#10;            batch_size_per_GPU = int(np.ceil(global_batch_size / world_size))&#10;&#10;            sample_id_low = 0 if my_rank == 0 else np.sum([batch_size_per_GPU] * my_rank)&#10;            sample_id_high = np.sum([batch_size_per_GPU] * (my_rank + 1))&#10;            # 截断到 global_batch_size 避免越界&#10;            sample_id_high = min(sample_id_high, global_batch_size)&#10;&#10;            oversample = [True if not i &lt; round(global_batch_size * (1 - self.oversample_foreground_percent)) else False&#10;                          for i in range(global_batch_size)]&#10;&#10;            if sample_id_low &gt;= global_batch_size:  # 多余进程（极端情况下）&#10;                print(f&quot;Rank {my_rank}: 没有可分配样本，进程将空转。建议减少GPU或增大batch size。&quot;)&#10;                self.batch_size = 0&#10;                self.oversample_foreground_percent = 0.0&#10;                return&#10;&#10;            effective_span = sample_id_high - sample_id_low&#10;            if effective_span &lt; batch_size_per_GPU:&#10;                batch_size_per_GPU = effective_span&#10;&#10;            if sample_id_high / global_batch_size &lt; (1 - self.oversample_foreground_percent):&#10;                oversample_percent = 0.0&#10;            elif sample_id_low / global_batch_size &gt; (1 - self.oversample_foreground_percent):&#10;                oversample_percent = 1.0&#10;            else:&#10;                oversample_percent = sum(oversample[sample_id_low:sample_id_high]) / max(1, batch_size_per_GPU)&#10;&#10;            print(f&quot;DDP模式 - worker {my_rank}: global_batch {global_batch_size}, perGPU {batch_size_per_GPU}, oversample {oversample_percent:.3f} (factor={batch_size_factor})&quot;)&#10;            self.batch_size = batch_size_per_GPU&#10;            self.oversample_foreground_percent = oversample_percent&#10;&#10;    def run_training(self):&#10;        &quot;&quot;&quot;覆写，增加tqdm进度条 (仅rank0)。通过环境变量 PROGRESS_BAR=1 开启。&quot;&quot;&quot;&#10;        use_bar = os.environ.get('PROGRESS_BAR', '0').lower() in ('1', 'true', 'yes', 'y')&#10;        if use_bar:&#10;            try:&#10;                from tqdm import tqdm&#10;            except ImportError:&#10;                if self.local_rank == 0:&#10;                    print('未安装tqdm，关闭进度条。 pip install tqdm 可开启。')&#10;                use_bar = False&#10;        self.on_train_start()&#10;        for epoch in range(self.current_epoch, self.num_epochs):&#10;            self.on_epoch_start()&#10;            self.on_train_epoch_start()&#10;            train_outputs = []&#10;            if use_bar and self.local_rank == 0:&#10;                iter_obj = tqdm(range(self.num_iterations_per_epoch), desc=f'Epoch {epoch} [train]', leave=False)&#10;            else:&#10;                iter_obj = range(self.num_iterations_per_epoch)&#10;            for batch_id in iter_obj:&#10;                train_outputs.append(self.train_step(next(self.dataloader_train)))&#10;            self.on_train_epoch_end(train_outputs)&#10;            with torch.no_grad():&#10;                self.on_validation_epoch_start()&#10;                val_outputs = []&#10;                if use_bar and self.local_rank == 0:&#10;                    val_iter = tqdm(range(self.num_val_iterations_per_epoch), desc=f'Epoch {epoch} [val]', leave=False)&#10;                else:&#10;                    val_iter = range(self.num_val_iterations_per_epoch)&#10;                for batch_id in val_iter:&#10;                    val_outputs.append(self.validation_step(next(self.dataloader_val)))&#10;                self.on_validation_epoch_end(val_outputs)&#10;            self.on_epoch_end()&#10;        self.on_train_end()&#10;&#10;    @staticmethod&#10;    def build_network_architecture(plans_manager: PlansManager,&#10;                                   dataset_json,&#10;                                   configuration_manager: ConfigurationManager,&#10;                                   num_input_channels,&#10;                                   enable_deep_supervision: bool = True) -&gt; nn.Module:&#10;        &quot;&quot;&quot;&#10;        构建带有多种注意力机制的3D UNet网络&#10;        &#10;        网络深度可以通过环境变量 UNET_3D_STAGES 控制:&#10;        - -1 (默认): 自动根据patch size和GPU内存优化&#10;        - 4-7: 指定具体的网络阶段数&#10;        &#10;        注意力机制可以通过环境变量控制:&#10;        - ENCODER_ATTENTION: 编码器注意力机制 (&quot;coord_attention&quot;, &quot;mamba&quot;, &quot;enhanced_mamba&quot;, &quot;none&quot;)&#10;        - DECODER_ATTENTION: 解码器注意力机制 (&quot;cbam&quot;, &quot;coord_attention&quot;, &quot;se_block&quot;, &quot;eca&quot;, &quot;sa&quot;, &quot;self_attention&quot;)&#10;        &#10;        特点:&#10;        1. 集成多种注意力机制增强特征表示&#10;        2. 自动调整网络深度以适应3D任务&#10;        3. 优化特征图数量平衡精度和内存使用&#10;        &quot;&quot;&quot;&#10;        # 获取原始配置参数&#10;        original_num_stages = len(configuration_manager.conv_kernel_sizes)&#10;        dim = len(configuration_manager.conv_kernel_sizes[0])&#10;        conv_op = convert_dim_to_conv_op(dim)&#10;&#10;        label_manager = plans_manager.get_label_manager(dataset_json)&#10;&#10;        # 验证是否为3D任务&#10;        if dim != 3:&#10;            raise ValueError(&quot;此训练器专为3D分割任务设计，当前任务不是3D任务&quot;)&#10;        &#10;        # 获取目标阶段数（网络深度）&#10;        target_num_stages_env = int(os.environ.get('UNET_3D_STAGES', -1))&#10;        &#10;        # 获取注意力类型&#10;        encoder_attention = os.environ.get('ENCODER_ATTENTION', 'coord_attention')&#10;        decoder_attention = os.environ.get('DECODER_ATTENTION', 'cbam')&#10;        &#10;        if target_num_stages_env == -1:&#10;            # 自动调整网络深度，针对3D任务优化&#10;            patch_size = configuration_manager.patch_size&#10;            volume = np.prod(patch_size)  # 3D patch的体素数量&#10;            &#10;            # 根据3D patch体积确定合适的网络深度&#10;            if volume &lt; 64*64*64:  # 相对较小的patch&#10;                target_num_stages = 5&#10;            elif volume &lt; 128*128*128:  # 中等大小patch&#10;                target_num_stages = 6&#10;            else:  # 大型patch&#10;                target_num_stages = min(7, original_num_stages)&#10;        else:&#10;            # 使用指定的阶段数&#10;            target_num_stages = max(4, min(target_num_stages_env, 8))&#10;        &#10;        # 确保不超过原始配置的阶段数&#10;        target_num_stages = min(target_num_stages, original_num_stages)&#10;        &#10;        print(f&quot;3D UNet with Multi Attention配置:&quot;)&#10;        print(f&quot;  原始阶段数={original_num_stages}, 目标阶段数={target_num_stages}&quot;)&#10;        print(f&quot;  编码器注意力机制={encoder_attention}, 解码器注意力机制={decoder_attention}&quot;)&#10;        &#10;        # 针对3D任务优化特征图数量，避免显存不足&#10;        base_num_features = configuration_manager.UNet_base_num_features&#10;        max_num_features = configuration_manager.unet_max_num_features&#10;        &#10;        # 对于3D任务，适当减少特征图数量以节省显存&#10;        if target_num_stages &gt;= 6:&#10;            base_num_features = min(base_num_features, 24)  # 减少基础特征数&#10;            max_num_features = min(max_num_features, 256)   # 减少最大特征数&#10;        &#10;        features_per_stage = [min(base_num_features * 2 ** i, max_num_features) for i in range(target_num_stages)]&#10;        print(f&quot;特征图数量: {features_per_stage}&quot;)&#10;        &#10;        # 调整卷积核大小和步长配置&#10;        conv_kernel_sizes = configuration_manager.conv_kernel_sizes[:target_num_stages]&#10;        pool_op_kernel_sizes = configuration_manager.pool_op_kernel_sizes[:target_num_stages]&#10;        &#10;        # 针对3D任务优化卷积块数量&#10;        # 更深的网络减少每个阶段的卷积层数以控制计算复杂度&#10;        if hasattr(configuration_manager, 'n_conv_per_stage_encoder'):&#10;            n_conv_per_stage_encoder = configuration_manager.n_conv_per_stage_encoder[:target_num_stages]&#10;        else:&#10;            # 默认每个阶段2个卷积层&#10;            n_conv_per_stage_encoder = [2] * target_num_stages&#10;            &#10;        if hasattr(configuration_manager, 'n_conv_per_stage_decoder'):&#10;            n_conv_per_stage_decoder = configuration_manager.n_conv_per_stage_decoder[:target_num_stages-1]&#10;        else:&#10;            # 默认每个阶段2个卷积层&#10;            n_conv_per_stage_decoder = [2] * (target_num_stages - 1)&#10;            &#10;        # 对于更深的网络，减少部分阶段的卷积层数&#10;        if target_num_stages &gt;= 6:&#10;            # 减少最后几个阶段的卷积层数以控制计算量&#10;            for i in range(max(0, target_num_stages-2), target_num_stages):&#10;                n_conv_per_stage_encoder[i] = min(n_conv_per_stage_encoder[i], 2)&#10;&#10;        # 构建网络&#10;        model = PlainConvUNetWithMultiAttention(&#10;            input_channels=num_input_channels,&#10;            n_stages=target_num_stages,&#10;            features_per_stage=features_per_stage,&#10;            conv_op=conv_op,&#10;            kernel_sizes=conv_kernel_sizes,&#10;            strides=pool_op_kernel_sizes,&#10;            num_classes=label_manager.num_segmentation_heads,&#10;            n_conv_per_stage=n_conv_per_stage_encoder,&#10;            n_conv_per_stage_decoder=n_conv_per_stage_decoder,&#10;            conv_bias=True,&#10;            norm_op=get_matching_instancenorm(conv_op),&#10;            norm_op_kwargs={'eps': 1e-5, 'affine': True},&#10;            dropout_op=None,&#10;            dropout_op_kwargs=None,&#10;            nonlin=nn.LeakyReLU,&#10;            nonlin_kwargs={'inplace': True},&#10;            deep_supervision=enable_deep_supervision,&#10;            encoder_attention=encoder_attention,&#10;            decoder_attention=decoder_attention&#10;        )&#10;        &#10;        return model" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>